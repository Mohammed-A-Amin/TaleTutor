{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Created\n",
      "trasncript: Ah, my dear student, let me tell you a tale from the wizarding world of Harry Potter to help you understand the role of political parties. Imagine if Hogwarts was run by different houses, each representing a political party. The Gryffindor party believes in bravery and courage, the Slytherin party values ambition and cunning, the Hufflepuff party stands for loyalty and hard work, and the Ravenclaw party values intelligence and wisdom.\n",
      "\n",
      "Now, let me ask you, why do you think it's important for Hogwarts to have these different houses, just like how a country has different political parties? What do you think each house or party brings to the table in terms of leadership and decision-making?\n",
      "Ah, my dear student, let me tell you a tale from the wizarding world of Harry Potter to help you understand the role of political parties. Imagine if Hogwarts was run by different houses, each representing a political party. The Gryffindor party believes in bravery and courage, the Slytherin party values ambition and cunning, the Hufflepuff party stands for loyalty and hard work, and the Ravenclaw party values intelligence and wisdom.\n",
      "\n",
      "Now, let me ask you, why do you think it's important for Hogwarts to have these different houses, just like how a country has different political parties? What do you think each house or party brings to the table in terms of leadership and decision-making?\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import json\n",
    "from text2speech import *\n",
    "from core_knowledge_llm_RAG import *\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "#os.getenv(\"OPENAI_API_KEY\")\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "\n",
    "movie=\"Harry Potter\"\n",
    "subject= None # [\"Physics\", \"Civics\"]\n",
    "chapter= None # [\"Newton's Laws of Motion\", \"Role of Political Parties\"]\n",
    "chapter_pdf_mapping= {\n",
    "\"Newton's Laws of Motion\":r\"/Users/mohammedamin/Desktop/Projects/calhacks_narration_learning/nextjs-flask/app/pages/teacherupload/Newtons_laws.pdf\" ,\n",
    "\"Role of Political Parties\": r\"/Users/mohammedamin/Desktop/Projects/calhacks_narration_learning/nextjs-flask/app/pages/teacherupload/Civics_Role_Of_Political_Parties.pdf\"\n",
    "} #CHANGED\n",
    "initial_context= None\n",
    "chat_history= None\n",
    "vectorstore= None\n",
    "model= None\n",
    "st_name= None\n",
    "movie_mapping= {\"Harry Potter\": \"Dumbledore\"}\n",
    "\n",
    "# Defining the knowledge_llm_func's input schema\n",
    "class knowledge_llm_func_schema(BaseModel):\n",
    "    user_query: str= Field(...,description= \"Query asked by the user\")\n",
    "\n",
    "\n",
    "@tool(args_schema=knowledge_llm_func_schema)\n",
    "def knowledge_llm_calling(user_query: str) -> str:\n",
    "    \"\"\"A function that can only answer questions on Physics subject on the Newton's Law of Motion chapter\"\"\"\n",
    "    global subject, chapter, movie, initial_context, chat_history, vectorstore,chapter_pdf_mapping, model, st_name\n",
    "    context= RAG_inference(question= user_query, vectorstore= vectorstore, model= model)\n",
    "    return context\n",
    "\n",
    "# Define the initial story model \n",
    "story_mode= ChatOpenAI(#model= ,\n",
    "    openai_api_key= os.getenv(\"OPENAI_API_KEY\"), \n",
    "    temperature=0, \n",
    "    #prompt=prompt\n",
    "    )\n",
    "\n",
    "def initial_user_input(st_name_user, subject_user, chapter_user, theme):\n",
    "    global subject, chapter, movie, initial_context, chat_history, vectorstore,chapter_pdf_mapping, model, st_name\n",
    "    subject= subject_user\n",
    "    chapter= chapter_user\n",
    "    st_name= st_name_user\n",
    "    movie= theme\n",
    "    vectorstore, model= model_embeddings_vectordatabase(pdf= chapter_pdf_mapping[chapter_user], key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    print(\"Vector Store Created\")\n",
    "    if(chapter==\"Role of Political Parties\"):\n",
    "        question= f\"In detail explain the Role of Political Parties: Power, Reporting, and Influence on Central Government Decisions?\"\n",
    "    else:\n",
    "        question= f\"In detail explain the idea of all three Newton's Law of Motion?\" #CHANGED\n",
    "\n",
    "    initial_context= RAG_inference(question= question, vectorstore= vectorstore, model= model,key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "    #initial_context= knowledge_llm_calling({'user_query': \"Explain in detail, what is {chapter}? \", 'subject': subject, 'chapter': chapter})\n",
    "    \n",
    "    prompt= ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \n",
    "        f\"\"\"\n",
    "    Imagine you are {movie_mapping[movie]} from the {movie} movie and you are going to teach me (your student) about {chapter}. Use the below context to teach:\n",
    "\n",
    "    Context: {initial_context}\n",
    "\n",
    "    Your teaching style:\n",
    "    * Teach concepts by narrating a story happening in the {movie} universe using characters from the movie.\n",
    "    * Fore every new question or new concept you teach, try to continue the same story.\n",
    "    * Use {movie_mapping[movie]}'s slang and narrate the story from {movie_mapping[movie]}'s POV.\n",
    "    * Treat me as a 10 year old kid\n",
    "    * Narrate a story and ask questions in between to guide me in the right path to learn the concepts. Occasionally, pause and conduct easy quiz to make sure I've understood the concepts correctly and  re-explain using if I'm wrong.\n",
    "    * After asking a question, wait for my response and ONLY PROCEED AFTER GETTING THE RESPONSE.\n",
    "    * Do not send more than 100 words in every iteration.\"\"\"\n",
    "        )\n",
    "    ])\n",
    "    chain_story_model= prompt | story_mode #| OpenAIFunctionsAgentOutputParser() | route_to_knowledge_llm    \n",
    "    story_llm_response =chain_story_model.invoke({\"chapter\":chapter,\"movie\": movie,\"initial_context\":initial_context})\n",
    "    chat_history = f\"AI: {story_llm_response.content}\"\n",
    "    trasncript= story_llm_response.content\n",
    "    print(\"trasncript:\", trasncript)\n",
    "    #audio_path= text_to_speech(trasncript)\n",
    "    #print(\"audio_path:\", audio_path)\n",
    "    return trasncript\n",
    "    \n",
    "   \n",
    "print(initial_user_input(\"Niranjan\", \"Civics\", \"Role of Political Parties\", \"Harry Potter\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For reply with \n",
    "story_mode1= ChatOpenAI(#model= ,\n",
    "    openai_api_key= os.getenv(\"OPENAI_API_KEY\"), \n",
    "    temperature=0, \n",
    "    #prompt=prompt\n",
    "    ).bind_tools(tools= [knowledge_llm_calling])\n",
    "\n",
    "prompt1= ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"If the user's input is a response to the your previous question in the mentioned chat history, continue the story. If the user has asked a new question from the {chapter}, ONLY THEN call the knowledge_llm_calling function. If the user's response is irrelevant to the subject and previous conversation, remind to get back to the topic and gently continue\"),\n",
    "    (\"user\", \"Chat History is: {chat_history} User's input is:{user_input}\")    \n",
    "])\n",
    "\n",
    "chain_story_model1= prompt1 | story_mode1 #| OpenAIFunctionsAgentOutputParser() | route_to_knowledge_llm\n",
    "\n",
    "# For reply with knowledge LLM's result\n",
    "story_mode2= ChatOpenAI(#model= ,\n",
    "    openai_api_key= os.getenv(\"OPENAI_API_KEY\"), \n",
    "    temperature=0, \n",
    "    )\n",
    "\n",
    "prompt2= ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You have been teaching the user in Dumbledore's slang and the user has asked you the below provided question. See if the below provided context can be used to answer the question. If yes, answer it. BUT ONLY IF THE CONTEXT IS NOT SUFFICIENT, say the teacher will address that question and continue the previous conversation.\"),\n",
    "    (\"user\", \"Chat History is: {chat_history} \\n\\n User's question is: {user_input} \\n\\n Context is: {new_content}\")    \n",
    "])\n",
    "\n",
    "chain_story_model2= prompt2 | story_mode2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function call for the subsequent responses\n",
    "def chat_user_input(user_input):\n",
    "    global subject, chapter, movie, initial_context, chat_history, vectorstore,chapter_pdf_mapping, model, st_name\n",
    "    # Inference 1\n",
    "    story_llm_response1 =chain_story_model1.invoke({'user_input':user_input , \"chapter\":chapter,\"subject\":subject,\"chat_history\":chat_history})\n",
    "    #story_llm_response =chain.invoke({'user_input': \"I know this concept, can you teach me what's Newton's Third Law please?\", \"chapter\":chapter,\"subject\":subject,\"chat_history\":chat_history })\n",
    "    #story_llm_response = chain.invoke({'user_input': \"Maybe Harry stay in the same place?\", \"chapter\":chapter,\"subject\":subject,\"chat_history\":chat_history })\n",
    "\n",
    "    if (story_llm_response1.content==''):\n",
    "        # Call the knowledge model function\n",
    "        function_name= story_llm_response1.additional_kwargs['tool_calls'][0]['function']['name']\n",
    "        function_args= json.loads(story_llm_response1.additional_kwargs['tool_calls'][0]['function']['arguments'])\n",
    "        #print(function_args)\n",
    "        print(\"Performing Rag\")\n",
    "        new_content= knowledge_llm_calling(function_args)\n",
    "        story_llm_response2 =chain_story_model2.invoke({'user_input':user_input , 'new_content': new_content, \"chapter\":chapter,\"subject\":subject,\"chat_history\":chat_history,'movie':movie})\n",
    "        chat_history += f\"\\n\\n Human:{user_input} \\n\\nAI: {story_llm_response2.content}\"\n",
    "        trasncript= story_llm_response2.content\n",
    "        \n",
    "        # Create teacher ticket\n",
    "        if (\"teacher\" in trasncript):\n",
    "            ticket_data= {\"STUDENT_NAME\": st_name, \n",
    "             \"TOPIC\": subject,\n",
    "             \"CHAPTER\": chapter,\n",
    "             \"QUESTION\":chat_history,\n",
    "             \"CONVERSATION_HISTORY\": chat_history,\n",
    "             \"STATUS\": \"Unresolved\"\n",
    "            }\n",
    "\n",
    "           \n",
    "            file_path= r'/Users/mohammedamin/Desktop/Projects/calhacks_narration_learning/nextjs-flask/app/pages/teacherupload/teacher_question_ticket.xlsx'\n",
    "            # Add ticket_data as a new row and \n",
    "            print(\"Creating Teacher Ticket\")\n",
    "            if not os.path.exists(file_path):\n",
    "                ticket_df = pd.DataFrame([ticket_data])\n",
    "                ticket_df.to_excel(file_path, index=False)\n",
    "            else:\n",
    "                ticket_df = pd.read_excel(file_path)\n",
    "                ticket_data= pd.DataFrame([ticket_data])\n",
    "                ticket_df = pd.concat([ticket_df, ticket_data], ignore_index=True)\n",
    "                ticket_df.to_excel(file_path, index=False)     \n",
    "        #audio_path= text_to_speech(trasncript)\n",
    "        #print(\"audio_path:\", audio_path)\n",
    "        print(\"trasncript:\", trasncript)\n",
    "        return trasncript\n",
    "    else:\n",
    "        # Update the chat history\n",
    "        chat_history = str(chat_history) + f\"\\n\\n Human:{user_input} \\n\\nAI: {story_llm_response1.content}\"\n",
    "        trasncript= story_llm_response1.content\n",
    "        #audio_path= text_to_speech(trasncript)\n",
    "        #print(\"audio_path:\", audio_path)\n",
    "        print(\"trasncript:\", trasncript)\n",
    "        return trasncript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Rag\n",
      "Creating Teacher Ticket\n",
      "trasncript: The teacher will address that question.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The teacher will address that question.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_user_input(\"Can you instead teach me about the reporting and accountability responsibility of different parties?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted URL: https://peregrine-results.s3.amazonaws.com/pigeon/exjSMzOcwFXfK3OPXp_0.mp3\n",
      "trasncript: That's an interesting question! However, we were discussing Newton's Laws of Motion. Let's get back to our topic. If you have any questions related to Newton's Laws of Motion, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's an interesting question! However, we were discussing Newton's Laws of Motion. Let's get back to our topic. If you have any questions related to Newton's Laws of Motion, feel free to ask!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_user_input(\"Can you teach me the history of Newton's life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
